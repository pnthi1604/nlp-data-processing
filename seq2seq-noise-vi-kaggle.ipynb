{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8560740,"sourceType":"datasetVersion","datasetId":5116898}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# set up","metadata":{}},{"cell_type":"code","source":"%mkdir /kaggle/working/dataset\n%mkdir /kaggle/working/dataset/tokenizer\n%mkdir /kaggle/working/dataset/statistic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/pnthi1604/nlp_data_processing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /opt/conda/lib/python3.10/site-packages/aiohttp*\n!pip install --force-reinstall --no-deps aiohttp==3.9.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install contractions\n!pip install bs4\n!pip install underthesea","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# import","metadata":{}},{"cell_type":"code","source":"from nlp_data_processing.utils.mapping import (\n    separate_text,\n    separate_word,\n    normalize_punctuation_spacing,\n    contraction,\n    word_tokenize_vn,\n    separate_text_with_min_max_len,\n)\n\nfrom nlp_data_processing.utils.statistic import (\n    draw_graph,\n    draw_hist_graph,\n    get_length_tokens,\n)\n\nfrom nlp_data_processing.utils.save import (\n    write_file,\n)\n\nfrom nlp_data_processing.utils.filter import (\n    condition_length_with_tokenizer,\n    condition_non_number_character,\n    condition_min_max_length,\n)\n\nfrom nlp_data_processing.utils.tokenizers import (\n    ApiTokenizerHuggingFace,\n    read_tokenizer,\n    BPE_TOKEN,\n    WORDPIECE_TOKEN,\n    WORDLEVEL_TOKEN,\n)\n\nfrom nlp_data_processing.utils.seed import set_seed\n\nfrom nlp_data_processing.utils.create_noise import (\n    token_masking,\n    token_deletion,\n    document_rotation,\n    text_infilling,\n    sentence_permutation,\n    TOKEN_MASKING,\n    TOKEN_DELETION,\n    DOCUMENT_ROTATION,\n    TEXT_INFILLING,\n    SENTENCE_PERMUTATION,\n)\n\nimport pandas as pd\nimport torch\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# config","metadata":{}},{"cell_type":"code","source":"config = {}\nGET_NOISE_FN = {\n    TOKEN_MASKING: token_masking,\n    TOKEN_DELETION: token_deletion,\n    DOCUMENT_ROTATION: document_rotation,\n    TEXT_INFILLING: text_infilling,\n    SENTENCE_PERMUTATION: sentence_permutation,\n}\n\n\nconfig[\"max_sample\"] = 10000000000\nconfig[\"max_get_sample\"] = 10000000\n# config[\"max_sample\"] = 10\n# config[\"max_get_sample\"] = 25\n\nconfig[\"raw_data_path\"] = \"/kaggle/input/dataset/bart/pretrain/raw_data/dataset.csv\"\nconfig[\"train_data_path\"] = \"/kaggle/working/dataset/train.csv\"\n\nconfig[\"min_len_token\"] = 6\nconfig[\"max_len_token\"] = 196\nconfig[\"ranges\"] = [(6, 100, 0.2), (101, 196, 0.8)] # (min, max, ratio)\n\nconfig[\"lang_src\"] = \"noise_vi\"\nconfig[\"lang_tgt\"] = \"vi\"\n\nconfig[\"vocab_size_src\"] = 35000\nconfig[\"vocab_size_tgt\"] = 35000\nconfig[\"min_frequency\"] = 2\nconfig[\"seed\"] = 42\nconfig[\"special_tokens\"] = [\n    \"<s>\",\n    \"</s>\",\n    \"<pad>\",\n    \"<unk>\",\n    \"<mask>\",\n    \"<cls>\",\n    \"<sep>\",\n]\nconfig[\"type_token_src\"] = WORDPIECE_TOKEN\nconfig[\"type_token_tgt\"] = WORDPIECE_TOKEN\n# TOKEN_MASKING\n# config[\"type_noise\"] = TOKEN_MASKING\n# config[\"ratio\"] = 0.3\n\n# TOKEN_DELETION\n# config[\"type_noise\"] = TOKEN_DELETION\n# config[\"ratio\"] = 0.15\n\n# DOCUMENT_ROTATION\n# config[\"type_noise\"] = DOCUMENT_ROTATION\n# config[\"ratio\"] = 1\n\n# TEXT_INFILLING\n# config[\"type_noise\"] = TEXT_INFILLING\n# config[\"ratio\"] = 0.2\n\n# SENTENCE_PERMUTATION\n# config[\"type_noise\"] = SENTENCE_PERMUTATION\n# config[\"ratio\"] = 1\n\n# Shuffle\nconfig[\"type_noise\"] = [TEXT_INFILLING, SENTENCE_PERMUTATION]\nconfig[\"ratio\"] = [0.3, 1]\n\nconfig[\"train_tokenizer\"] = True\nconfig[\"tokenizer_src_path\"] = \"/kaggle/working/dataset/tokenizer/tokenizer_src.json\"\nconfig[\"tokenizer_tgt_path\"] = \"/kaggle/working/dataset/tokenizer/tokenizer_tgt.json\"\n\nconfig[\"graph_len_token_src_path\"] = \"/kaggle/working/dataset/statistic/len_token_src.png\"\nconfig[\"graph_len_token_tgt_path\"] = \"/kaggle/working/dataset/statistic/len_token_tgt.png\"\n\nconfig[\"desc_path\"] = \"/kaggle/working/dataset/desc.txt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(seed=config[\"seed\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# read raw data","metadata":{}},{"cell_type":"code","source":"raw_data = pd.read_csv(config[\"raw_data_path\"])[:config[\"max_sample\"]]\nraw_data = raw_data.dropna()\nraw_data = raw_data.drop_duplicates()\nraw_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data = raw_data.rename(columns={\"Contents\": config[\"lang_src\"]})\nraw_data = raw_data[[config[\"lang_src\"]]]\nraw_data = raw_data.drop_duplicates()\nraw_data = raw_data.dropna()\nraw_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# split sentence","metadata":{}},{"cell_type":"code","source":"new_raw_dataset = None\nfor min_len, max_len, ratio in config[\"ranges\"]:\n    tmp_raw_data = raw_data.copy()\n    tmp_raw_data[config[\"lang_src\"]] = tmp_raw_data[config[\"lang_src\"]].apply(lambda text: separate_text_with_min_max_len(\n        text=text,\n        min_len=min_len,\n        max_len=max_len,\n    ))\n    tmp_raw_data = tmp_raw_data.explode(config[\"lang_src\"])\n    max_sample = int(config[\"max_get_sample\"] * ratio)\n    tmp_raw_data = tmp_raw_data[:max_sample]\n    if new_raw_dataset is None:\n        new_raw_dataset = tmp_raw_data\n    else:\n        new_raw_dataset = pd.concat([new_raw_dataset, tmp_raw_data])\n\nraw_data = new_raw_dataset\nraw_data[config[\"lang_tgt\"]] = raw_data[config[\"lang_src\"]]\nraw_data.reset_index(drop=True, inplace=True)\nraw_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# filter number character","metadata":{}},{"cell_type":"code","source":"# raw_data = raw_data[raw_data.apply(\n#     lambda text: condition_non_number_character(\n#         text=text[config[\"lang_src\"]],\n#     ) and condition_non_number_character(\n#         text=text[config[\"lang_tgt\"]],\n#     ),\n#     axis=1,\n# )]\n# raw_data.reset_index(drop=True, inplace=True)\n# raw_data = raw_data[:int(config[\"max_get_sample\"] * 1.1)]\n# raw_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# filter number words","metadata":{}},{"cell_type":"code","source":"raw_data = raw_data[raw_data.apply(\n    lambda text: condition_min_max_length(\n        text=text[config[\"lang_src\"]],\n        min_len=config[\"min_len_token\"],\n        max_len=config[\"max_len_token\"],\n    ) and condition_min_max_length(\n        text=text[config[\"lang_tgt\"]],\n        min_len=config[\"min_len_token\"],\n        max_len=config[\"max_len_token\"],\n    ),\n    axis=1,\n)]\nraw_data.reset_index(drop=True, inplace=True)\nraw_data = raw_data[:int(config[\"max_get_sample\"] * 1.1)]\nraw_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# normalize data","metadata":{}},{"cell_type":"code","source":"def mapping_item(item):\n    return  normalize_punctuation_spacing(item.lower()).strip()\n\nraw_data = raw_data[:int(config[\"max_get_sample\"] * 1.1)]\n    \nraw_data[config[\"lang_src\"]] = raw_data[config[\"lang_src\"]].map(lambda item: mapping_item(item))\nraw_data[config[\"lang_tgt\"]] = raw_data[config[\"lang_tgt\"]].map(lambda item: mapping_item(item))\nsum_item = 0\nraw_datas = []\nwhile sum_item < int(config[\"max_get_sample\"] * 1.1):\n    tmp_data = raw_data\n    type_noises = config[\"type_noise\"]\n    ratios = config[\"ratio\"]\n    for i in range(len(type_noises)):\n        type_noise = type_noises[i]\n        ratio = ratios[i]\n        noise_fn = GET_NOISE_FN[type_noise]\n        raw_data[config[\"lang_src\"]] = raw_data[config[\"lang_src\"]].map(lambda item: noise_fn(\n            text=item,\n            ratio=ratio,\n        ))\n    raw_datas.append(raw_data)\n    sum_item += len(raw_data)\n    raw_data = tmp_data\n\nfor i in range(len(raw_datas)):\n    raw_datas[i] = raw_datas[i].drop_duplicates()\n    raw_datas[i] = raw_datas[i].dropna()\n    raw_datas[i].reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# build tokenizer for dataset","metadata":{}},{"cell_type":"code","source":"dataset = raw_datas[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config[\"train_tokenizer\"]:  \n    trainer_tokenizer_src = ApiTokenizerHuggingFace(\n        dataset=dataset[config[\"lang_src\"]],\n        vocab_size=config[\"vocab_size_src\"],\n        min_frequency=config[\"min_frequency\"],\n        special_tokens=config[\"special_tokens\"],\n        type_token=config[\"type_token_src\"],\n    )\n\n    trainer_tokenizer_tgt = ApiTokenizerHuggingFace(\n        dataset=dataset[config[\"lang_tgt\"]],\n        vocab_size=config[\"vocab_size_tgt\"],\n        min_frequency=config[\"min_frequency\"],\n        special_tokens=config[\"special_tokens\"],\n        type_token=config[\"type_token_tgt\"],\n    )\n\n    # train tokenizer\n    tokenzier_src = trainer_tokenizer_src.train()\n    tokenzier_tgt = trainer_tokenizer_tgt.train()\n\n    # save tokenizer\n    tokenzier_src.save(config[\"tokenizer_src_path\"])\n    tokenzier_tgt.save(config[\"tokenizer_tgt_path\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read tokenizer\ntokenizer_src, tokenizer_tgt = read_tokenizer(\n    tokenizer_src_path=config[\"tokenizer_src_path\"],\n    tokenizer_tgt_path=config[\"tokenizer_tgt_path\"],\n)\n\nconfig[\"vocab_size_src\"] = tokenizer_src.get_vocab_size()\nconfig[\"vocab_size_tgt\"] = tokenizer_tgt.get_vocab_size()\n\nprint(\"Vocab size src: \", config[\"vocab_size_src\"])\nprint(\"Vocab size tgt: \", config[\"vocab_size_tgt\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# fillter length tokens","metadata":{}},{"cell_type":"code","source":"for i in range(len(raw_datas)):\n    raw_datas[i] = raw_datas[i][raw_datas[i].apply(\n        lambda text: condition_length_with_tokenizer(\n            tokenizer=tokenizer_src,\n            text=text[config[\"lang_src\"]],\n            min_len_token=config[\"min_len_token\"],\n            max_len_token=config[\"max_len_token\"],\n        ) and condition_length_with_tokenizer(\n            tokenizer=tokenizer_tgt,\n            text=text[config[\"lang_tgt\"]],\n            min_len_token=config[\"min_len_token\"],\n            max_len_token=config[\"max_len_token\"],\n        ),\n        axis=1,\n    )]\nprint(\"Success\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.concat(raw_datas, ignore_index=True)\ndataset = dataset[:config[\"max_get_sample\"]]\ndataset.reset_index(drop=True, inplace=True)\n\n# save dataset\ndataset.to_csv(config[\"train_data_path\"], index=False)\n\n# read dataset\ndataset = pd.read_csv(config[\"train_data_path\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lenght_data_src = get_length_tokens(\n    tokenizer=tokenizer_src,\n    dataset=dataset[config[\"lang_src\"]],\n)\n\nlenght_data_tgt = get_length_tokens(\n    tokenizer=tokenizer_tgt,\n    dataset=dataset[config[\"lang_tgt\"]],\n)\n\nconfig[\"min_len_token\"] = min(lenght_data_src + lenght_data_tgt)\nconfig[\"max_len_token\"] = max(lenght_data_src + lenght_data_tgt)\n\ndraw_hist_graph(\n    title=\"Histogram length tokens\",\n    xlabel=\"Length tokens\",\n    ylabel=\"Frequency\",\n    data=lenght_data_src,\n    save_path=config[\"graph_len_token_src_path\"],\n)\ndraw_hist_graph(\n    title=\"Histogram length tokens\",\n    xlabel=\"Length tokens\",\n    ylabel=\"Frequency\",\n    data=lenght_data_tgt,\n    save_path=config[\"graph_len_token_tgt_path\"],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# save desc","metadata":{}},{"cell_type":"code","source":"config[\"length_dataset\"] = len(dataset)\nconfig[\"desc\"] = f\"Vocab size src: {config['vocab_size_src']}\\nVocab size tgt: {config['vocab_size_tgt']}\\nMin frequency: {config['min_frequency']}\\nMin len token: {config['min_len_token']}\\nMax len token: {config['max_len_token']}\\nType token src: {config['type_token_src']}\\nType token tgt: {config['type_token_tgt']}\\nSpecial tokens: {config['special_tokens']}\\nLength dataset: {config['length_dataset']}\\nType nosie: {config['type_noise']}\\nRatio: {config['ratio']}\\nRanges: {config['ranges']}\"\nwrite_file(\n    file_name=config[\"desc_path\"],\n    content=config[\"desc\"],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}