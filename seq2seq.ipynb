{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8560740,"sourceType":"datasetVersion","datasetId":5116898}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# set up","metadata":{}},{"cell_type":"code","source":"%mkdir /kaggle/working/dataset\n%mkdir /kaggle/working/dataset/tokenizer\n%mkdir /kaggle/working/dataset/statistic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/pnthi1604/nlp_data_processing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install contractions\n!pip install bs4\n!pip install underthesea","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# import","metadata":{}},{"cell_type":"code","source":"from nlp_data_processing.utils.mapping import (\n    separate_text,\n    separate_word,\n    normalize_punctuation_spacing,\n    contraction,\n    word_tokenize_vn,\n)\n\nfrom nlp_data_processing.utils.statistic import (\n    draw_graph,\n    draw_hist_graph,\n    get_length_tokens,\n)\n\nfrom nlp_data_processing.utils.save import (\n    write_file,\n)\n\nfrom nlp_data_processing.utils.filter import (\n    condition_length_with_tokenizer,\n    condition_non_number_character,\n    condition_min_max_length,\n)\n\nfrom nlp_data_processing.utils.tokenizers import (\n    ApiTokenizerHuggingFace,\n    read_tokenizer,\n    BPE_TOKEN,\n    WORDPIECE_TOKEN,\n    WORDLEVEL_TOKEN,\n)\n\nfrom nlp_data_processing.utils.seed import set_seed\n\nfrom nlp_data_processing.utils.create_noise import (\n    token_masking,\n    token_deletion,\n    document_rotation,\n    text_infilling,\n    TOKEN_MASKING,\n    TOKEN_DELETION,\n    DOCUMENT_ROTATION,\n    TEXT_INFILLING,\n)\n\nimport pandas as pd\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# seed","metadata":{}},{"cell_type":"code","source":"set_seed(seed=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# config","metadata":{}},{"cell_type":"code","source":"config = {}\nGET_NOISE_FN = {\n    TOKEN_MASKING: token_masking,\n    TOKEN_DELETION: token_deletion,\n    DOCUMENT_ROTATION: document_rotation,\n    TEXT_INFILLING: text_infilling,\n}\n\n\nconfig[\"max_sample\"] = 100000000000000000\nconfig[\"max_get_sample\"] = 5000000\n\nconfig[\"raw_data_path\"] = \"/kaggle/input/dataset/bart/pretrain/raw_data/dataset.csv\"\nconfig[\"train_data_path\"] = \"/kaggle/working/dataset/train_data.csv\"\n\nconfig[\"min_len_token\"] = 9\nconfig[\"max_len_token\"] = 96\n\nconfig[\"lang_src\"] = \"noise_vi\"\nconfig[\"lang_tgt\"] = \"vi\"\n\nconfig[\"vocab_size_src\"] = 35000\nconfig[\"vocab_size_tgt\"] = 35000\nconfig[\"min_frequency\"] = 2\nconfig[\"special_tokens\"] = [\n    \"<s>\",\n    \"<pad>\",\n    \"</s>\",\n    \"<unk>\",\n    \"<mask>\",\n    \"<cls>\",\n    \"<sep>\",\n]\nconfig[\"type_token_src\"] = WORDPIECE_TOKEN\nconfig[\"type_token_tgt\"] = WORDPIECE_TOKEN\n# TOKEN_MASKING\n# TOKEN_DELETION\n# DOCUMENT_ROTATION\n# TEXT_INFILLING\nconfig[\"type_noise\"] = TEXT_INFILLING\nconfig[\"ratio\"] = 0.2\n\nconfig[\"tokenizer_src_path\"] = \"/kaggle/working/dataset/tokenizer/tokenizer_src.json\"\nconfig[\"tokenizer_tgt_path\"] = \"/kaggle/working/dataset/tokenizer/tokenizer_tgt.json\"\n\nconfig[\"graph_len_token_src_path\"] = \"/kaggle/working/dataset/statistic/len_token_src.png\"\nconfig[\"graph_len_token_tgt_path\"] = \"/kaggle/working/dataset/statistic/len_token_tgt.png\"\n\nconfig[\"desc_path\"] = \"/kaggle/working/dataset/desc.txt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# read raw data","metadata":{}},{"cell_type":"code","source":"raw_data = pd.read_csv(config[\"raw_data_path\"])[:config[\"max_sample\"]]\nraw_data = raw_data.dropna()\nraw_data = raw_data.drop_duplicates()\nraw_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data = raw_data.rename(columns={\"Contents\": config[\"lang_src\"]})\nraw_data = raw_data[[config[\"lang_src\"]]]\nraw_data = raw_data.drop_duplicates()\nraw_data = raw_data.dropna()\nraw_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# split sentence","metadata":{}},{"cell_type":"code","source":"raw_data[config[\"lang_src\"]] = raw_data[config[\"lang_src\"]].apply(separate_text)\nraw_data = raw_data.explode(config[\"lang_src\"])\nraw_data[config[\"lang_tgt\"]] = raw_data[config[\"lang_src\"]]\nraw_data.reset_index(drop=True, inplace=True)\nraw_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# filter number character","metadata":{}},{"cell_type":"code","source":"raw_data = raw_data[raw_data.apply(\n    lambda text: condition_non_number_character(\n        text=text[config[\"lang_src\"]],\n    ) and condition_non_number_character(\n        text=text[config[\"lang_tgt\"]],\n    ),\n    axis=1,\n)]\nraw_data.reset_index(drop=True, inplace=True)\nraw_data = raw_data[:int(config[\"max_get_sample\"] * 1.1)]\nraw_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# filter number words","metadata":{}},{"cell_type":"code","source":"raw_data = raw_data[raw_data.apply(\n    lambda text: condition_min_max_length(\n        text=text[config[\"lang_src\"]],\n        min_len=config[\"min_len_token\"],\n        max_len=config[\"max_len_token\"],\n    ) and condition_min_max_length(\n        text=text[config[\"lang_tgt\"]],\n        min_len=config[\"min_len_token\"],\n        max_len=config[\"max_len_token\"],\n    ),\n    axis=1,\n)]\nraw_data.reset_index(drop=True, inplace=True)\nraw_data = raw_data[:int(config[\"max_get_sample\"] * 1.1)]\nraw_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# normalize data","metadata":{}},{"cell_type":"code","source":"def mapping_item(item):\n    return  contraction(normalize_punctuation_spacing(item.lower())).strip()\n\nif config[\"max_get_sample\"] <= 4000000:\n    raw_data = pd.concat([raw_data, raw_data], ignore_index=True)\nelse:  \n    raw_data = pd.concat([raw_data, raw_data, raw_data], ignore_index=True)\n    \nraw_data[config[\"lang_src\"]] = raw_data[config[\"lang_src\"]].map(lambda item: mapping_item(item))\nraw_data[config[\"lang_tgt\"]] = raw_data[config[\"lang_tgt\"]].map(lambda item: mapping_item(item))\nif config[\"lang_src\"] == \"noise_vi\":\n    noise_fn = GET_NOISE_FN[config[\"type_noise\"]]\n    ratio = config[\"ratio\"]\n    raw_data[config[\"lang_src\"]] = raw_data[config[\"lang_src\"]].map(lambda item: noise_fn(\n        text=item,\n        ratio=ratio,\n    ))\nraw_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# build tokenizer for dataset","metadata":{}},{"cell_type":"code","source":"dataset = raw_data[[config[\"lang_src\"], config[\"lang_tgt\"]]]\ndataset = dataset.drop_duplicates()\ndataset = dataset.dropna()\ndataset.reset_index(drop=True, inplace=True)\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer_tokenizer_src = ApiTokenizerHuggingFace(\n    dataset=dataset[config[\"lang_src\"]],\n    vocab_size=config[\"vocab_size_src\"],\n    min_frequency=config[\"min_frequency\"],\n    special_tokens=config[\"special_tokens\"],\n    type_token=config[\"type_token_src\"],\n)\n\ntrainer_tokenizer_tgt = ApiTokenizerHuggingFace(\n    dataset=dataset[config[\"lang_tgt\"]],\n    vocab_size=config[\"vocab_size_tgt\"],\n    min_frequency=config[\"min_frequency\"],\n    special_tokens=config[\"special_tokens\"],\n    type_token=config[\"type_token_tgt\"],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train tokenizer\ntokenzier_src = trainer_tokenizer_src.train()\ntokenzier_tgt = trainer_tokenizer_tgt.train()\n\n# save tokenizer\ntokenzier_src.save(config[\"tokenizer_src_path\"])\ntokenzier_tgt.save(config[\"tokenizer_tgt_path\"])\n\n# read tokenizer\ntokenizer_src, tokenizer_tgt = read_tokenizer(\n    tokenizer_src_path=config[\"tokenizer_src_path\"],\n    tokenizer_tgt_path=config[\"tokenizer_tgt_path\"],\n)\n\nconfig[\"vocab_size_src\"] = tokenzier_src.get_vocab_size()\nconfig[\"vocab_size_tgt\"] = tokenzier_tgt.get_vocab_size()\n\nprint(\"Vocab size src: \", config[\"vocab_size_src\"])\nprint(\"Vocab size tgt: \", config[\"vocab_size_tgt\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# fillter length tokens","metadata":{}},{"cell_type":"code","source":"dataset = dataset[dataset.apply(\n    lambda text: condition_length_with_tokenizer(\n        tokenizer=tokenizer_src,\n        text=text[config[\"lang_src\"]],\n        min_len_token=config[\"min_len_token\"],\n        max_len_token=config[\"max_len_token\"],\n    ) and condition_length_with_tokenizer(\n        tokenizer=tokenizer_tgt,\n        text=text[config[\"lang_tgt\"]],\n        min_len_token=config[\"min_len_token\"],\n        max_len_token=config[\"max_len_token\"],\n    ),\n    axis=1,\n)]\n\ndataset = dataset[:config[\"max_get_sample\"]]\ndataset.reset_index(drop=True, inplace=True)\n\n# save dataset\ndataset.to_csv(config[\"train_data_path\"], index=False)\n\n# read dataset\ndataset = pd.read_csv(config[\"train_data_path\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lenght_data_src = get_length_tokens(\n    tokenizer=tokenizer_src,\n    dataset=dataset[config[\"lang_src\"]],\n)\n\nlenght_data_tgt = get_length_tokens(\n    tokenizer=tokenizer_tgt,\n    dataset=dataset[config[\"lang_tgt\"]],\n)\n\nconfig[\"min_len_token\"] = min(lenght_data_src + lenght_data_tgt)\nconfig[\"max_len_token\"] = max(lenght_data_src + lenght_data_tgt)\n\ndraw_hist_graph(\n    title=\"Histogram length tokens\",\n    xlabel=\"Length tokens\",\n    ylabel=\"Frequency\",\n    data=lenght_data_src,\n    save_path=config[\"graph_len_token_src_path\"],\n)\ndraw_hist_graph(\n    title=\"Histogram length tokens\",\n    xlabel=\"Length tokens\",\n    ylabel=\"Frequency\",\n    data=lenght_data_tgt,\n    save_path=config[\"graph_len_token_tgt_path\"],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# save desc","metadata":{}},{"cell_type":"code","source":"config[\"length_dataset\"] = len(dataset)\nconfig[\"desc\"] = f\"Vocab size src: {config['vocab_size_src']}\\nVocab size tgt: {config['vocab_size_tgt']}\\nMin frequency: {config['min_frequency']}\\nMin len token: {config['min_len_token']}\\nMax len token: {config['max_len_token']}\\nType token src: {config['type_token_src']}\\nType token tgt: {config['type_token_tgt']}\\nSpecial tokens: {config['special_tokens']}\\nLength dataset: {config['length_dataset']}\\nType nosie: {config['type_noise']}\\nRatio: {config['ratio']}\"\nwrite_file(\n    file_name=config[\"desc_path\"],\n    content=config[\"desc\"],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}